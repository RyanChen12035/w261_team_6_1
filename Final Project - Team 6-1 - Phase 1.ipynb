{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b6bc60d-2afa-4ff8-8ee9-fbe83bed901d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Predicting Flight Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4560bd5e-8f81-41fa-867d-b08f84fd4fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Team Profile\n",
    "\n",
    "| Name             | E-mail                     | Photo       |\n",
    "|------------------|----------------------------|-------------|\n",
    "| Jason Rudianto   | jasonrudianto@berkeley.edu | <img src=https://dchansew.github.io/w261/Team_6-1_Jason_Rudianto.jpg width=\"200\"> |\n",
    "| Derrick Chan-Sew | dchansew@berkeley.edu      | <img src=https://dchansew.github.io/w261/Team_6-1_Derrick_Chan-Sew.jpg width=\"200\"> |\n",
    "| Sean Condon      | scondon@berkeley.edu       | <img src=https://dchansew.github.io/w261/Team_6-1_Sean_Condon.png width=\"200\"> |\n",
    "| Ryan Chen        | bread12035@berkeley.edu    | <img src=https://dchansew.github.io/w261/Team_6-1_Ryan_Chen.png width=\"200\"> |\n",
    "| Jessica Huber    | jesshuber@berkeley.edu     | <img src=https://dchansew.github.io/w261/Team_6-1_Jessica_Huber.jpg width=\"200\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef95947a-6257-453f-baf4-f740fb826516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Phase Leader Plan\n",
    "\n",
    "| Phase | Leader                      |\n",
    "|:-----:|-----------------------------|\n",
    "| 1     | Derrick Chan-Sew            |\n",
    "| 2     | Jessica Huber, Ryan Chen    |\n",
    "| 3     | Sean Condon, Jason Rudianto |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "273b103f-f8a0-448a-98cd-6a2a164e66a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Credit Assignment Plan\n",
    "\n",
    "| Name             | Workload                                                                                                                   |\n",
    "|------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Jason Rudianto   | Feature Engineering, Algorithm Exploration, Metrics, EDA, Data - Flights                                                   |\n",
    "| Derrick Chan-Sew | Abstract, EDA - OTPW, Feature Engineering, Algorithms, Gantt                                                               |\n",
    "| Sean Condon      | EDA, Feature Engineering, Algorithm Exploration, Data Analysis/Description - Weather Conclusions & Next Steps, Open Issues |\n",
    "| Ryan Chen        | Storage on Databricks setup, Feature Engineering, Algorithm Exploration, Project workspace organization,  Data - Stations  |\n",
    "| Jessica Huber    | Approach, Metrics, Pipeline, Feature Engineering, Algorithm Exploration                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6709704d-4643-498b-adb4-f1e8fe6f5fba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Abstract\n",
    "\n",
    "Airline flight delays can have a significant impact for passengers and the airline industry, leading to inconveniences, increased operational costs and decreased customer satisfaction. These delays create logistical issues that ultimately can impact revenues and increase the cost of air travel as a whole. In this research study, we examine domestic flights, station and weather time-series data to identify the causes of flight delays. Key fields of interest include weather conditions such as temperature, precipitation, wind and visibility. We will first explore the underlying data sets individually then perform joint analysis on the datasets by region and timeframe. Feature engineering and selection processes will identify the most relevant predictors and we will explore various techniques for handling missing data, categorical data and outliers. We will develop models using machine learning methods such as logistic regression, random forest, gradient boosted decision tree (GDBT), and neural networks and evaluate the performance of the models against historical data to develop the best model for future predictions. The findings of this study will offer valuable insight to airlines, airport authorities and air traffic management, enabling development of more effective delay mitigation strategies, operational improvements and better decision-making in the aviation industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710073da-67e1-448a-ac1a-a6b83756f31c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11727a0e-9955-4c28-bc49-317612212dba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Flights\n",
    "\n",
    "The flight data is a table with 109 features detailing flight information for 2,806,942 flights over a 3 month time frame. Some of the information that is supplied per flight includes the date of the flight, airline details, departure and arrival locations and times, times of significant events (such as wheels off) and more. Many of the columns are dependent values depending on other columns – for example, the column ARR_DELAY, ARR_DELAY_NEW, ARR_DEL15, and ARR_DELAY_GROUP are dependent on the independent variables CRS_ARR_TIME and ARR_TIME. \n",
    "\n",
    "\n",
    "Among the 109 features, over 70 features have a non-zero percentage of null values within their respective column. A majority of these 70 features have a significantly high percentage of null values – over 50+ of them have a null-value percentage of 75+%. Many of these values can be attributed to either those values only being present for applicable flights (i.e CANCELLATION_CODE if the flight was canceled). Likewise, some of the low null percentage columns can be attributed to non-applicable flight records, such as ARR_TIME or AIR_TIME for canceled flights.\n",
    "\n",
    "\n",
    "Included below are features that contain null values but of low percentage, as well as their percentage of null values. \n",
    "\n",
    "<br/>\n",
    "<img src=https://dchansew.github.io/w261/Team_6-1_EDA_flights_features_percent_null.png width=\"300\">\n",
    "\n",
    "Some general features that are of interest are: \n",
    "1. (The season of the year the flight took place)\n",
    "2. OP_CARRIER_AIRLINE_ID (Airline company)\n",
    "3. ORIGIN_AIRPORT_ID (ID of the departing airport)\n",
    "4. DEST_AIRPORT_ID (ID of the destination airport)\n",
    "5. CANCELLED (Whether the flight was canceled)\n",
    "6. DEP_TIME_BLK (Departure flight time hour segment)\n",
    "7. ARR_TIME_BLK (Arrival flight time hour segment)\n",
    "8. DISTANCE (Distance of the flight)\n",
    "9. DEP_DELAY (Time delta between estimated departure time and actual)\n",
    "10. ARR_DELAY (Time delta between estimated arrival time and actual)\n",
    "\n",
    "\n",
    "As mentioned earlier, there are also other features that are dependent on other variables which could be leveraged instead. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12675a8d-204b-481c-9dec-a52f63fe2aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Weather\n",
    "\n",
    "Exploratory data analysis was done on the 3 month weather dataset which has 124 columns and 30 million rows. The weather data has the following categories of columns:\n",
    "\n",
    "\n",
    "* Metadata:\n",
    "    * Station, Date, Latitude, Longitude, Elevation, Name, Report type, Source, Year, Sunrise, Sunset\n",
    "    * NormalsCoolingDegreeDay, NormalsHeatingDegreeDay\n",
    "* Backup readings\n",
    "    * E.g. BackupElevation\n",
    "* Monthly, daily, and hourly weather data for the following:\n",
    "    * Temperature, precipitation, present weather type, pressure, humidity, sky conditions, visibility, wind\n",
    "* Short duration precipitation readings and associated end dates\n",
    "\n",
    "\n",
    "The metadata can be used to join rows of this data set to the stations dataset. We have 12,500 stations here, and only a subset will be close enough to airports to provide useful weather information.\n",
    "\n",
    "\n",
    "We also note that weather can change rapidly throughout the day, so we will stick with the “Hourly*” weather columns, and drop the monthly and daily values. The most relevant columns to predict flight delays will be:\n",
    "\n",
    "* Metadata (like Station) for joining to the stations and flights data sets\n",
    "* Hourly weather readings, especially (but not limited to):\n",
    "    * Precipitation\n",
    "    * PresentWeatherType\n",
    "    * SkyConditions\n",
    "    * Visibility\n",
    "    * WindSpeed, WindGustSpeed\n",
    "* Short duration precipitation readings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4f57dbc-0239-4d63-a01f-5c6e1e51e0b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Stations\n",
    "\n",
    "The station data set incorporates the location of the weather station and airport, their neighbor airports, and the distance between each station. In this data set, we have 2,237 stations including itself and they are all fully connected which means passengers can take flight from any of the airports to other airports. Furthermore, there are no duplicates and invalid data like null or NaN in the data set. Percentage of null or NaN in each column is as follows:\n",
    "\n",
    "<img src=https://dchansew.github.io/w261/Team_6-1_EDA_stations_features_null.png width=\"1000\">\n",
    "\n",
    "Among all states, Alaska (AK) has the highest number of airports and stations: 173 airports, and California (CA) has 145 which is reasonable due to size or population. Also, combined with weather data, we can point out that flights from or to the state having bad weather may be delayed. Furthermore, The most distanced airports on average are in Hawaii and Alaska which may also have a higher probability of being affected by weather from or to the destination. The columns below may contain helpful information for flight delay analysis.\n",
    "1. station_id:\n",
    "2. lat,lon:\n",
    "3. neighbor_id\n",
    "4. neighbor_call\n",
    "5. neighbor_lat, neighbor_lon\n",
    "6. Distance_to_neighbor\n",
    "\n",
    "A brief summary of distance from one airport to another is as follows:\n",
    "\n",
    "<img src=https://dchansew.github.io/w261/Team_6-1_EDA_stations_avg_distance_to_neighbor.png width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47bd92c7-0a4c-42f4-9321-772916763bab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Exploratory Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450e3a76-01fe-4977-8e17-a5b8ce178030",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Flights\n",
    "\n",
    "One of the features that we want to look at is the distribution of arrival flights delays. The data shows a vast majority of flights with no delays and even if there are delays, it tends to be very short. As such, the distribution of flight delay times (both exclusive and inclusive of non-delayed flights) skews rightward. There are approximately 1.09 million flights with delays and among those, there is a mean delay of 33 minutes, median delay of 16 minutes, and a high standard deviation due to high counts of outliers. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src= https://dchansew.github.io/w261/Team_6_1_EDA_flights_quad_charts.png width=\"750\">\n",
    "\n",
    "Additionally, we can inspect the delays by the originating airport and compute the mean delay. We were able to identify the airport with the highest mean delay is PPG (Pago Pago INTL Airports in the American Samoa).\n",
    "\n",
    "Moving away from flight delays and inspecting cancellations instead, we were able to identify the top 5 origin airports with the most canceled flights being EWR, BOS, LGA, ORD, DFW with ORD and DFW having over 7000 recorded canceled flights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379e01a9-d1cb-4d75-b918-d821003334c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Weather\n",
    "\n",
    "Upon analyzing the null value rate for the weather data columns of interest in the section above, we found a >99.99% null value rate for the ShortDuration columns. After dropping those, we are left with the metadata columns (which all have a null value rate < 1%), and the hourly weather columns, which have reasonable null value rates shown in the plot below:\n",
    "\n",
    "![Team_6_1_EDA_weather_percent_hourly.png](https://dchansew.github.io/w261/Team_6_1_EDA_weather_percent_hourly.png)\n",
    "\n",
    "Some of these columns show null value rates above 70%, so we will likely need to impute values for these missing rows. For example, we could reasonably take a null wind gust speed to mean 0 wind gust, or a null present weather type to mean clear. More on this in coming weeks, once we do a full EDA + feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3076ed8-a3ec-433c-aeba-ae561d2ea1bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Stations\n",
    "\n",
    "The following is the map of international airports in the US and represents the distribution of airports:\n",
    "\n",
    "![Team_6_1_USA_stations.png](https://dchansew.github.io/w261/Team_6-1_USA_stations.png)\n",
    "\n",
    "In terms of feature engineering, the distribution of the distance from a specific airport to other airports may be skewed. As a result, for training purposes, we may need to take log transformation on these data set. Here we take the airport that has the smallest average distance to others, a geographic center, as an example. The histograms of the distribution of Pella Municipal Airports to other airports and after log transformation are as follows: \n",
    "\n",
    "<img src=https://dchansew.github.io/w261/Team_6_1_EDA_stations_bi_chart_PMA.png width=\"750\">\n",
    "\n",
    "Next, we take the busiest airport: Hartsfield-Jackson Atlanta International Airport as an example. We can see the distribution on the right histogram is close to normal distribution and it’s beneficial for model training.\n",
    "\n",
    "<img src=https://dchansew.github.io/w261/Team_6_1_EDA_stations_ATL.png width=\"750\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd50fc9a-2257-4d4e-879a-fec73afd7e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Machine Learning Algorithms\n",
    "\n",
    "We plan to explore the following classification models:\n",
    "* __Logistic Regression__: Logistic regression is highly interpretable making it easy to understand how each feature contributes to flight delays.  This will be our baseline model to compare the performance of the more complex algorithms.  Logistic regression is well-suited for our large dataset and high dimensional feature spaces do not add much computational overhead.\n",
    "  * Loss Function: Binary Cross-Entropy Loss\n",
    "* __Random Forest__: Random forest generalizes well to a complex dataset.  Specifically, it could do a good job of capturing any non-linear relationships between the features and delays.  Random forest can also provide feature importance scores that would highlight the most influential features in predicting flight delays.  We could then feed this information back to feature engineering.\n",
    "  * Loss Function: None, final classification is based on majority voting but impurity measures like Gini or Entropy are used during training for individual decision trees.\n",
    "* __Gradient Boosted Decision Tree__: GDBT is known for its high predictive power.  It can handle noisy data and outliers and is more immune to overfitting compared to single decision trees.  GDBT can handle the feature selection for us.\n",
    "  * Loss Function: Binomial Deviance\n",
    "* __MLP Neural Network__: A MLP could learn the complex relationships between our features and flight delays.  It is also flexible in the data types we can use.  There is a lot of opportunity for improved performance given we can try different architectures.\n",
    "  * Loss Function: Binary Cross-Entropy Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3915d81b-d114-4787-bc33-9955c487c7df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Metrics\n",
    "\n",
    "\n",
    "| Metrics              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Accuracy             | Equation: (TP + TN) / (TP + FP + TN + FN) <br/><br/> Accuracy will measure the proportion of correctly predicted flights (on-time - TN, delayed - TP) and a high accuracy would indicate our model is making accurate decisions which is important for improving the customer experience.                                                                                                                                                                                                |\n",
    "| Precision            | Equation: TP / (TP + FP)  <br/><br/>  Precision will measure the proportion of correctly predicted delayed flights from the total number of predicted delays.  A high precision would mean when our model predicts a delayed flight, it is likely to be accurate.  This is very important for reducing Type 1 error which is mistakenly telling customers their flight is delayed when it is in fact on-time.  This could cause customers to miss their flight which is the worst outcome. |\n",
    "| F1-Score             | Equation: 2 * (Precision * Recall) / (Precision + Recall)  <br/><br/>  F1 is the harmonic mean of precision and recall.  A high F1-score would mean the model is effectively balancing minimizing false delays and capturing actual delays.                                                                                                                                                                                                                                                |\n",
    "| Sensitivity (Recall) | Equation: TP / (TP + FN)  <br/><br/>  Sensitivity measures the model’s ability to correctly predict actual delayed flights.  A high sensitivity would ensure the model minimizes the risk of missing actual delays (Type II Error).  While this is important type II error will be less detrimental to customers compared to Type I Error                                                                                                                                                  |\n",
    "| Specificity          | Equation: TN / (TN + FP)  <br/><br/>  Specificity measures the proportion of correctly predicted on-time flights from the total number of actual on-time flights.  A high specificity would help reduce the chance of incorrectly predicting a delay when the flight is in fact on-time.                                                                                                                                                                                                                                                                                                                                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d4e445a-88ee-4868-bdd5-fd62e2d3e94b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Project Plan\n",
    "\n",
    "![Gantt](https://dchansew.github.io/w261/Team_6-1_Gantt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3870d08-5fd6-4819-8e1f-898ad8ba1615",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "<img src=https://dchansew.github.io/w261/Team_6-1_Pipeline.png width=\"800\">\n",
    "\n",
    "__Raw Data Extraction__\n",
    "* Retrieve the flight data, weather data, and station data\n",
    "* Load the datasets into the pipeline for preprocessing.\n",
    "\n",
    "\n",
    "__EDA / Preprocessing__\n",
    "* Descriptive statistics for numerical data and create graphical representations to understand the data better\n",
    "* Identify outliers and missing values\n",
    "* Encode categorical features\n",
    "* Scale numerical features\n",
    "\n",
    "__Feature Engineering__\n",
    "* Feature transformations (standardization, normalization, etc.)  and dimensionality reduction\n",
    "* New feature creation (recency, frequency, monetary)\n",
    "* Feature selection\n",
    "\n",
    "__Data Splitting__\n",
    "* Divide dataset for training, validation, and testing\n",
    "\n",
    "__Model Training and Selection__\n",
    "* Choose the machine learning model (logistic regression, random forest, gradient boosted decision tree, and neural networks)\n",
    "* Train the the models with the training dataset and measure performance throughout training with validation dataset\n",
    "* Retrain model with different features and hyperparameters\n",
    "\n",
    "__Hyperparameter Tuning__\n",
    "* Adjust the hyperparameters to optimize the model performance (grid search, random search)\n",
    "\n",
    "__Model Evaluation and Comparison__\n",
    "* Model is used on the test dataset\n",
    "* Evaluate the models performance using accuracy, precision, F1-Score, sensitivity, and specificity\n",
    "* Compare the performance of the different models to determine which one best meets our project goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a809efbd-b272-407c-a20d-0433eebe851d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Cross Validation\n",
    "\n",
    "Unlike the typical cross-validation, time-series cross-validation maintains observation order so splitting validation data randomly may not work for validating model performance in a time-series case. For example, it’s odd to validate a model trained with winter data with summer validation data. There may be some seasonal effects across different time-series data sets. To tackle this issue, there are expanding and sliding methods to split the validation data. The former one is expanding validation data one month at a time in this case. For example, training with data from January to March and validating it with data from April. As for the latter, training with data from January to March and validating it using the data from January the following year then training with data from February to April and validating with data from February of the following year. Intuitively, we presume there is a strong seasonal effect in weather data so we decide to pick up the sliding method. However, we also need to take yearly incidents like El Niño into consideration. Next, if there is a special event that can potentially affect our training and validation process, we can create an indicator column  for the model to identify them based on the domain knowledge and other statistics. For instance, we can point out how long El Niño lasts or how long an economic recession would put the flight frequency down and reduce flight delays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4ed499-c77d-4092-bb7a-47181539931d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Open Issues or Problems\n",
    "\n",
    "The biggest open issue seems to be significant null value rates in columns of interest. It likely won’t be feasible to drop all rows with any null values in columns of interest, so we will need to investigate imputation for many of these columns. In the coming weeks, when we do a complete EDA and feature engineering phase, we should concentrate on finding reasonable imputed values for these columns.\n",
    "\n",
    "Another open issue is a large amount of categorical data. We will need to decide on how to transform categorical columns into numerical input, either by one-hot encoding, or transforming into metric data if the categories support it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb2ae72-6db3-4470-b1e8-ebde57086a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "The open issues listed above are thankfully common with machine learning problems, and we have plenty of useful data to sort them out. In the coming weeks, we will focus on the following:\n",
    "\n",
    "\n",
    "1. Continuing exploratory data analysis to decide on a final set of columns to use.\n",
    "2. Handling null values in those columns by imputation or removal\n",
    "3. Combining, scaling, and normalizing the final set of columns\n",
    "4. Splitting the data into a train, validation, and test set\n",
    "\n",
    "Further down the line, we will be able to apply our machine learning algorithms on the training set, and measure their performance using the metrics listed above on the validation and test set.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1084138578717633,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final Project - Team 6-1 - Phase 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
